# TLP: A Deep Learning-based Cost Model for Tensor Program Tuning

# TLP：一种基于深度学习的张量程序调优成本模型

---

## 关键词

张量、特征、成本模型

---

## 核心思想

调度原语序列和张量程序几乎是一一对应的，这是可行的。基于此，TLP的特征提取才是可行的。使用共享参数来拟合硬件无关特征，并使用非共享参数来拟合硬件相关特征

---

## 主要工作

 针对张量程序的一种简单、高效且通用的特征提取机制。一种解决跨硬件平台离线成本模型不可用问题的多任务学习方法。

---

## 学到的知识点

1. 张量程序调优：一个非凸目标优化问题，基于搜索的方法已被证明是有效的。基于搜索方法的核心是成本模型的设计。尽管基于深度学习的成本模型比其他方法表现得更好，但它们仍然存在不足，并存在以下问题：
   
   - 它们的特征提取严重依赖于硬件架构方面的专家级领域知识。即使如此，提取的特征往往不尽如人意，需要分别考虑 CPU 和 GPU。
   
   - 在一个硬件平台上训练的成本模型通常在另一个平台上表现不佳，我们称之为跨硬件不可用性。

2. **TLP**：TLP 是一种基于深度学习的张量程序调优成本模型，它可以促进张量程序调优。TLP 并没有从张量程序本身提取特征，而是从调度原语中提取特征。我们将调度原语视为张量语言。因此，TLP 是一种张量语言处理任务。通过这种方式，通过成本模型预测张量程序延迟的任务被转化为自然语言处理 (NLP) 回归任务。
   
   没有从张量程序源代码中提取特征的2个原因：
   
   - 张量程序的源代码是树形结构数据，包含嵌套循环，其中的 AST 信息难以提取和利用。
   
   - 源代码中存在大量无关的字符标记，导致特征稀疏。
   
   **作者从调度原语中提取特征，将调度原语视为张量语言，并将预测张量程序延迟的任务转化为 NLP 回归任务**。

3. 张量程序调优的主要挑战：测量张量程序延迟非常耗时。张量程序延迟的耗时测量是由于以下三个原因造成的：
   
   - 测量流程包含多个步骤，包括编译、加载和执行。如果使用远程过程调用方法，则需要两次网络传输。编译特别耗时，需要降低到多个中间表示 (IR) 并对这些 IR 进行广泛的优化。
   
   - 为了保证测量精度，通常需要多次重复测量。如果在 CPU 上进行测量，则需要在两次连续测量之间刷新缓存。
   
   - 测量任务通常会独占计算资源以避免噪声，从而阻止潜在的并行执行模式。
   
   耗时的难题使得在调优过程中将所有生成的张量程序放置在目标硬件平台上进行延迟测量变得不可能。为了解决这个问题，**许多深度学习编译器采用成本模型– 使用成本模型预测的延迟作为标准**。基于深度学习的成本模型比其他方法的性能要好得多。然而，现有的基于深度学习的成本模型严重依赖于复杂的特征工程。而复杂的特征工程可能存在问题
   
   - 有精通硬件架构的领域专家才能胜任这项工作。即使如此，手工挑选的成本模型在许多情况下仍然存在不足，很大程度上受到先验知识的限制。
   
   - 在 CPU 和 GPU 上提取的特征是不同的；即使在 CPU 上提取，特征也不能在不同架构的 CPU 上（例如 Intel、ARM 和 AMD CPU）同时获得稳定的性能。为了考虑不同的硬件架构，需要更多的工程努力，这使得特征提取成为一个费力且繁琐的过程。
   
   这个耗时的难题也阻碍了我们在所有可能的硬件平台上收集大规模张量程序数据集。此外，在离线训练的成本模型在另一个硬件平台上的性能会急剧下降，我们称之为**跨硬件离线成本模型不可用问题**。

4. 针对第3点中提出的问题，本文提出TLP 和 MTL-TLP。针对测量张量程序延迟非常耗时这个问题，提出TLP。针对离线成本模型在不同硬件之间不可用问题，使用多任务学习技术来解决。类似于张量编译器使用高级图 IR 和低级张量 IR 来抽象硬件无关和硬件相关特性的思想，**作者使用共享和非共享参数来拟合硬件无关和硬件相关特性**。一个张量程序在不同的硬件平台上具有不同的延迟（标签）。作者设置了多个任务，每个任务对应一个硬件平台。

5. TLP 和 MTL-TLP 适用于所有自动搜索框架，这些框架通过调度原语将高级图 IR 降低到低级张量 IR。

6. 常见的编译器搜索算法：遗传算法 (GA)、蒙特卡罗树搜索 (MCTS)、模拟退火。

7. **DL编译器（基于搜索的编译器）工作的一般流程**：
   
   - 接受深度学习工作负载作为输入（深度学习工作负载本质上是用各种高级编程语言编写的计算图。）
   
   - 经过一些计算图优化后，一些划分算法将长计算图划分为计算子图。每个子图都有自己的搜索空间（通过输入/输出张量形状、数据类型、数据布局和目标硬件平台，搜索空间通常在 CPU 上达到数百万级，在 GPU 上达到数十亿级。）
   
   - 在遗传算法 (GA)、蒙特卡罗树搜索 (MCTS)、模拟退火等算法的指导下，编译器将各种调度原语组合应用于每个子图。（在这个过程中，会生成数万个张量程序。）
   
   - 编译器使用成本模型过滤掉性能最佳的候选程序。然后将这些候选程序放到目标硬件上进行测量，以找到性能最高的张量程序。
   
   - 这些高性能张量程序最终被委托给特定的加速后端或成熟的 LLVM/CUDA 以生成最终的可执行文件。

8. TLP 训练和推理：
   
   - 训练：
     
     - 自动调优器根据接收到的计算子图生成多个调度原语序列
     
     - 自动调优器调用代码生成器，结合计算子图和调度原语序列生成张量程序，并在目标硬件上测量这些张量程序的延迟
     
     - TLP 以调度原语序列作为输入。经过预处理、特征提取和后处理后，TLP 成本模型正向传播最终提取的特征，并将对应张量程序的延迟归一化为标签，以计算损失
     
     - 损失被反向传播以更新成本模型的权重
   
   - 推理：
     
     - 在生成调度原语序列后，自动调优器通过成本模型获得预测分数
     
     - 根据预测分数筛选出前 k 个潜在候选者
     
     - 自动调优器最终为这些候选者生成张量程序，并在目标硬件上测量它们的延迟

9. 具体原语与抽象原语：在 TLP 的流水线中，具体原语是指自动搜索框架中的调度原语。抽象原语是指经过预处理后符合 TLP 特征提取器输入规范的原语。

10. **特征提取是构建成本模型的核心。它决定了成本模型所能达到的性能上限。**

11. TLP特征提取：TLP 是第一个直接从调度原语中提取特征的张量程序成本模型。从调度原语中有效地提取特征并保留所有有价值的信息非常具有挑战性。这里有三种方法：
    
    - 将调度原语序列视为字符文本，然后使用 NLP 任务进行文本处理
    
    - 将每个调度原语视为一个 NLP 词语，因此完整的调度原语序列就是一个 NLP 句子。然后，将每个词转换为一个标记
    
    - 将调度原语视为三个基本元素的组合——类型、数值参数和字符参数，即原语。（ TLP 采用）
    
    TLP 特征提取首先对输入的调度基元序列进行预处理。对于序列中的每个基元，TLP 特征提取仅保留三个基本元素：基元类型、数值参数和字符参数。无关字符将被去除。事实上，这三个元素基本上包含了调度基元中的所有语义信息。预处理算法的实现与特定的自动搜索框架有关。在大多数框架中，这种预处理算法是可逆的，即可以恢复调度原语序列。
    
    我们可以将一个原语视为一个 NLP 词语，并将编码结果视为经过嵌入层后的 NLP 任务的嵌入向量。TLP 特征提取是一种词嵌入算法，它在很大程度上保留了同义词关系。由于两个类型相同但参数不同的原语的编码结果在许多值上相同，因此归一化后两个原语之间的欧氏距离相对较短。此外，TLP 直接从调度原语中提取特征，无需生成张量程序，从而减少了调优时间。

12. **TLP 特征提取的可行性分析**：不同的张量程序的调度原语序列可能相同。然而，我们认为不同的张量程序具有相同调度原语序列的概率很低。经过作者大量实验，我们可以近似地认为，一个调度原语序列唯一地刻画了一个张量程序。也就是说，我们将调度原语序列等同于张量程序。

13. 调度原语序列的优势：
    
    - 一个张量程序可以用多个特征来描述。这些特征是稠密的，并且保留了所有信息
    
    - 调度原语序列是一个规则序列，可以利用一些现有的 NLP 技术
    
    - 调度原语的种类很少
    
    - 对于调度原语，我们不需要复杂的特征工程。我们只记录调度原语的类型和参数。这样就可以避免在 AST 上操作复杂的嵌套循环

14. 从张量程序到调度原语序列：调度原语序列和张量程序几乎是一一对应的，包含相同数量的语义信息。张量程序的源代码是嵌套循环树结构，而调度原语序列是规则的序列结构。从语言的角度来看，调度原语序列的结构与自然语言相同，两者都具有顺序结构。我们将调度原语称为张量语言。因此，TLP 是一种张量语言处理任务，它将预测张量程序延迟的成本模型任务转化为 NLP 回归任务。

15. 跨硬件不可用问题：由于硬件架构和硬件性能方面的领域差距，离线收集的数据在不同硬件之间无效。这导致离线训练的成本模型在不同硬件平台上的性能显著下降。经验公式成本模型和在线学习成本模型不会受到这个问题的影响，因为它们不使用离线数据集。作者认为多任务学习技术最适合解决跨硬件离线模型不可用的问题，原因如下：
    
    - 跨硬件不可用问题非常适合多任务学习的范围，从而减轻了由领域差异引起的模型性能下降
    
    - 多任务学习和张量编译器的多层 IR 共享共同的设计理念。张量编译器分别使用图 IR 和张量 IR 来抽象硬件无关和硬件相关的特征。多任务学习非常适合这里：它可以使用共享参数来拟合硬件无关特征，并使用非共享参数来拟合硬件相关特征
    
    - 作者实验结果表明在微调、多任务学习、训练本地模型和自监督学习中，多任务学习表现最佳

16. MTL-TLP：MTL-TLP 的模型结构与 TLP 相似。不同之处在于 MTL-TLP 设置了多个头部，即多个任务，每个任务对应一个硬件平台。![](C:\Users\MCQSW\AppData\Roaming\marktext\images\2024-10-17-21-27-43-image.png)                                 张量程序在 CPU 和 GPU 之间并不通用，因此我们不讨论跨 CPU 和 GPU 的多任务学习。

17. MTL-TLP的可行性分析：
    
    - MTL 有效地增加了我们用于训练模型的样本量。尽管目标平台上只有少量标记数据，但其他平台上确实存在大量数据。目标平台成本模型可以使用来自其他平台的数据作为辅助任务进行训练。
    
    - 在单任务学习中，梯度反向传播容易陷入局部最小值。在多任务学习中，不同任务的局部最小值位于不同的位置。不同任务之间的相互作用可以帮助隐藏层逃离局部最小值。
    
    - MTL 通过引入归纳偏差充当正则化器。多个任务在浅层共享权重，这可能会削弱网络的能力，减少网络过拟合，并提高泛化能力。
    
    - MTL-TLP 可以使用共享参数来拟合硬件无关特征，并使用非共享参数来拟合硬件相关特征。

---

## 个人思考

本论文给我最大的一个启示是，**若对于研究的问题，研究A优于研究B，即使A不等价于B，但若A等同于B是可行的，那么也是可以去尝试用A替代B的**。就正如作者将研究对象从张量程序变成了调度原语。严格意义上说，张量程序与调度原语不是等价的。但作者通过分析得出不同的张量程序具有相同调度原语序列的概率很低这个结论，并使用大量实验进行了验证，最后证明了调度原语作为张量程序等效性的可行性。这个研究对象的改变，是这篇论文的核心。

同时，多任务学习和张量编译器的多层 IR 共享共同的设计理念。张量编译器分别使用图 IR 和张量 IR 来抽象硬件无关和硬件相关的特征。多任务学习非常适合这里：它可以使用共享参数来拟合硬件无关特征，并使用非共享参数来拟合硬件相关特征。用共享参数表征硬件无关这一共享特征，用非共享参数表征硬件相关特征这一非共享特征。这也值得我去学习。
