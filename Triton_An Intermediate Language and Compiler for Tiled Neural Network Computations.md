# Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations

# Triton: 用于分块神经网络计算的中间语言和编译器

---

## 关键词

IR、多层次、自动调优

---

## 核心思想

Triton 是一个开源语言和编译器，专为高效表达和编译平铺神经网络计算而设计。通过在 LLVM-IR 中引入少量数据流和控制流扩展，Triton 实现了多层次的优化，达到了与现有供应商库相当的性能。同时，Triton-C 提供了更简洁的接口，便于使用参数化平铺变量开发新型神经网络架构的高效内核。其自动调优功能和多维瓦片支持，使得编译和优化过程更加灵活与高效。

---

## 主要工作

本文介绍了 Triton，一个用于表达和编译平铺神经网络计算为高效机器代码的开源语言和编译器。并证明了在 LLVM-IR 中添加少量数据流和控制流扩展可以实现各种平铺级别优化流程，这些流程共同导致了与供应商库相当的性能。同时提出了 Triton-C，这是一种更高级的语言，我们能够用它简洁地实现用于 CNN 的新型神经网络架构的高效内核。

---

## 学到的知识点

1. 针对 DNN 的各种 DSL 和编译器的开发，通常基于以下三种不同的方法之一：
   
   - 张量级 IR：Tensor-level IRs，被 XLA和 Glow用于将张量程序转换为预定义的 LLVM-IR 和 CUDA-C 操作模板（例如，张量收缩、逐元素操作等），使用模式匹配。
   
   - 多面体模型：The polyhedral model，被张量推导 (TC)和 Diesel用于参数化和自动化将一个或多个 DNN 层编译成 LLVM-IR 和 CUDA-C 程序。
   
   - 循环合成器：Loop synthesizers，被 Halide和 TVM用于将张量计算转换为循环嵌套，这些循环嵌套可以使用用户定义（尽管可能是参数化的）调度进行手动优化。

2. **Triton 的优势**：以增加编程工作量为代价，依赖于在传统编译管道中添加 tile-level 操作和优化。提供了比 XLA 和 Glow 更多的灵活性；与 TC 和 Diesel 相反，支持非仿射张量索引；自动推断可能的执行调度，否则必须手动指定给 Halide 或 TVM。

3.  Triton-C：一种类 C 语言，用于使用参数化平铺变量来表达张量程序。该语言的目的是为现有的 DNN 转编译器（例如 PlaidML、Tensor Comprehensions）和熟悉 CUDA 的程序员提供一个稳定的接口。Triton-C 的语法基于 ANSI C（更具体地说，是 CUDA-C），但进行了修改。

4. Triton-IR：一种基于 LLVM 的中间表示 (IR)，其目的是提供一个适合于瓦片级程序分析、转换和优化的环境。在该论文中，Triton-IR 程序是直接构建的。在最高级别，Triton-IR 程序由一个或多个称为模块的编译基本单元组成。这些模块彼此独立编译，最终由链接器聚合，链接器的作用是解析前向声明并适当地合并全局定义。每个模块本身由函数、全局变量、常量和其他杂项符号（例如，元数据、函数属性）组成。
   
   - 函数：riton-IR 函数定义包含返回值类型、函数名和可能为空的参数列表。如果需要，还可以添加额外的可见性、对齐和链接说明符。也可以指定函数属性（例如内联提示）和参数属性（例如只读、别名提示），使编译器后端能够通过例如更好地利用只读内存缓存来执行更激进的优化。该头部之后是一个由基本块列表组成的主体，这些基本块之间的相互依赖关系形成了函数的控制流图 (CFG)。
   
   - 基本块：基本块，根据定义，是直线代码序列，其结尾处只能包含所谓的终止指令（即分支、返回）。Triton-IR 使用静态单赋值 (SSA) 形式，这意味着每个基本块中的每个变量必须 (1) 只被赋值一次，以及 (2) 在使用之前被定义。这样，每个基本块隐式地定义了一个数据流图 (DFG)，其不同的路径对应于程序 SSA 表示中的使用-定义链。这种形式可以直接从抽象语法树 (AST) 中创建，如 [7] 所示。

5. 多维瓦片：Multi-dimensional tiles，Triton-IR 中数据流分析的核心，可以使用类似于 LLVM-IR 中向量声明的语法进行声明。

6. Triton-JIT：目标是通过一组机器无关和机器相关的传递，以及一个自动调整引擎，将 Triton-IR 程序简化并编译成高效的机器代码。
   
   - 与机器无关的阶段（Machine-Independent Passes）：
     
     - 预取：Pre-Fetching，循环内部的瓦片级内存操作可能存在问题，因为它们可能会导致严重的延迟，在没有足够的独立指令的情况下无法隐藏。然而，可以通过直接在 Triton-IR 中检测循环并在必要时添加适当的预取代码来缓解这个问题。
     
     - tile级窥孔优化：Tile-Level Peephole Optimization，Triton-IR 中存在着 tile 级别的操作，这为 peephole [29] 优化器提供了新的机会。
   
   - 与机器有关的阶段（Machine-Dependent Passes）：
     
     - 分层平铺：Hierarchical Tiling，嵌套平铺策略旨在将瓦片分解成微瓦片，最终分解成纳米瓦片，以尽可能紧密地适应机器的计算能力和内存层次结构。虽然这种技术在自动调优框架中被广泛使用，但 Triton-IR 的结构使得能够自动枚举和优化任何可表达程序的有效嵌套平铺配置（无需多面体机制）。
     
     - 内存合并：Memory Coalescing，当相邻线程同时访问相邻内存位置时，内存访问被称为合并。这很重要，因为内存通常以大块从 DRAM 中检索。由于 Triton-IR 程序是单线程的并且自动并行化，我们的编译器后端能够在每个微瓦片内部对线程进行排序，以便在可能的情况下避免非合并内存访问。这种策略减少了加载瓦片列所需的内存事务数量。
     
     - 共享内存分配：Shared Memory Allocation，具有高算术强度（例如，点积）的瓦片级操作可以从将它们的运算对象临时存储在快速共享内存中获益。共享内存分配阶段的目的是确定何时以及何地将瓦片存储到该空间。
     
     - 共享内存同步：Shared Memory Synchronization，对共享内存的读写操作是异步的。共享内存同步阶段的目标是自动在生成的 GPU 源代码中插入屏障，以保证程序的正确性。这可以通过使用以下数据流方程，利用前向数据流分析来检测读后写 (RAW) 和写后读 (WAR) 冲突来实现：![](C:\Users\MCQSW\AppData\Roaming\marktext\images\2024-10-15-12-23-18-image.png)

7. 自动调优器：Auto-tuner，传统的自动调优器通常依赖于手写的参数化代码模板来在预定义的工作负载上实现良好的性能。相比之下，Triton-JIT 可以通过简单地连接与上述每个优化过程相关的元参数，直接从 Triton-IR 程序中提取优化空间。

---

## 个人思考

Triton 的出现标志着深度学习编译器领域的一个重要进展，它通过引入多维平铺和自适应优化手段，为开发者提供了更高的灵活性和效率。与传统的张量级 IR 和多面体模型相比，Triton 不仅能够支持非仿射索引，还能自动推断调度，降低了开发者的工作量。这种设计思路让我意识到，现代深度学习模型的复杂性要求编译器必须具备更强的适应性和智能化能力。

此外，Triton-C 的类 C 语言接口使得熟悉 CUDA 的程序员可以更容易地过渡到使用 Triton，进一步推动了社区的接受度。自动调优器的引入，则为性能优化提供了新的可能性，能够在不同硬件上自动调优，这将极大地促进深度学习应用的普及。总体来看，Triton 不仅提升了计算性能，更为未来的深度学习编译器发展提供了重要的参考和启示。
