# The Deep Learning Compiler: A Comprehensive Survey

# 深度学习编译器：一项全面调查

---

## 主要工作

本论文主要对目前主流的深度学习编译器的类别、结构（前/后端与IR设计）以及优化做了一个总结，并提出了未来展望。

---

## 学到的知识点

1. DL编译器研究开发的动力：在不同的 DL 硬件上部署各种深度学习 (DL) 模型的难度，依赖库的缺点是它们通常落后于 DL 模型的快速发展，因此无法有效利用 DL 芯片。

2. DL框架：![](C:\Users\MCQSW\AppData\Roaming\marktext\images\2024-10-13-15-15-30-image.png)
- TensorFlow：  在所有 DL 框架中，TensorFlow 对语言接口的支持最为全面，包括 C++、Python、Java、Go、R 和 Haskell。TensorFlow Lite 旨在用于移动和嵌入式深度学习，并提供 Android 神经网络 API。为了降低 TensorFlow 的使用复杂性，Google 采用 Keras 作为 TensorFlow 核心的前端。此外，TensorFlow 中的 eager 模式应用了类似于 PyTorch 的方法来更好地支持动态计算图。

- Keras：一个高级神经网络库，用于快速构建 DL 模型，用纯 Python 编写。尽管本身不是 DL 框架，但 Keras 提供了一个高级 API，可与 TensorFlow、MXNet、Theano 和 CNTK 集成。借助 Keras，DL 开发人员只需几行代码即可构建神经网络。此外，Keras 可以与其他常见的 DL 包集成，例如 scikit-learn。然而，由于过度封装，Keras 的灵活性不够，这使得添加运算符或获取低级数据信息变得非常困难。

- PyTorch：Facebook 已将基于 Lua 的 DL 框架 Torch 重写为 Python，并在张量级别重构了所有模块，从而发布了 PyTorch。PyTorch 内嵌了用于在 Python 中构建动态数据流图的基元，其中控制流在 Python 解释器中执行。支持高效的图形执行和移动部署。

- Caffe/Caffe2：加州大学伯克利分校设计用于深度学习和图像分类的。Caffe 的简单性使得源代码易于扩展，这适合于开发人员进行深入分析。因此，Caffe 主要定位于研究领域。Caffe2 建立在原始 Caffe 项目的基础上。Caffe2 在代码结构上类似于 TensorFlow，但 API 更轻量级，更容易访问计算图中的中间结果。

- MXNet：旨在具有可扩展性，并且从减少数据加载和 I/O 复杂性的角度进行设计。MXNet 提供了不同的范例：声明式编程（如 Caffe 和 Tensorflow）以及命令式编程（如 PyTorch）。

- CNTK：可通过 Python、C++ 和 C# API 或其自己的脚本语言（即 BrainScript）使用。设计为易于使用且适用于生产中的海量数据，尚未支持 ARM 架构，这限制了其在移动设备上的使用。它使用与 TensorFlow 和 Caffe 类似的静态计算图，其中 DL 模型被视为通过有向图的一系列计算步骤。

- PaddlePaddle：PaddlePaddle 的最初设计类似于 Caffe，其中每个模型都可以表示为一组层。然而，PaddlePaddle v2 借鉴了 TensorFlow 的概念，将层分解为更细粒度的算子，从而支持更复杂的 DL 模型。PaddlePaddle Fluid 与 PyTorch 类似，因为它提供了自己的解释器，以避免 Python 解释器的性能限制。
3. ONNX：The Open Neural Network Exchange，开放神经网络交换格式。它定义了一个可扩展的计算图模型，因此不同 DL 框架构建的计算图可以轻松转换为 ONNX。借助 ONNX，模型在 DL 框架之间转换变得更加容易。

4. 深度学习硬件：分为3类。
   
   - 通用硬件：General-purpose Hardware，最具代表性的 DL 模型通用硬件是图形处理器 (GPU)，它通过多核架构实现了高并行性。可以通过硬件和软件优化来支持 DL 工作负载。
   
   - 专用硬件：Dedicated Hardware，专用硬件完全定制用于深度学习计算，以极大地提高性能和能效。最著名的专用 DL 硬件是 Google 的 TPU 系列。TPU 包括矩阵乘法器单元 (MXU)、统一缓冲区 (UB) 和激活单元 (AU)，由主机处理器使用 CISC 指令驱动。与 CPU 和 GPU 相比，TPU 仍然可编程，但使用矩阵作为基元，而不是向量或标量。专注于通过完全定制的电路设计来加速 DL 工作负载。
   
   - 神经形态硬件：Neuromorphic Hardware，神经形态芯片使用电子技术来模拟生物大脑。此类芯片的代表性产品有 IBM 的 TrueNorth 和英特尔的 Loihi。神经形态芯片复制了类似于脑组织的结构：神经元可以同时存储和处理数据。传统芯片将处理器和内存分布在不同的位置，而神经形态芯片通常具有许多微处理器，每个微处理器都具有少量局部内存。然而，神经形态芯片还远未达到大规模商用生产。尽管如此，在计算机科学领域，神经形态芯片有助于捕捉常规深度学习模型所忽略的快速、终身学习的过程。

5. 硬件特定 DL 代码生成器：Hardware-specific DL Code Generator，FPGA。

6. DL编译器的高级IR：也称为图IR，表示计算和控制流，并且与硬件无关。高级 IR 的目标是建立操作符和数据之间的控制流和依赖关系，并为图级优化提供一个接口。它还包含用于编译的丰富语义信息，并为定制操作符提供可扩展性。

7. DL编译器的低级IR：旨在针对不同硬件目标进行硬件特定优化和代码生成。因此，低级IR应该足够细粒度以反映硬件特性并表示硬件特定优化。

8. 前端：将现有 DL 框架中的 DL 模型作为输入，然后将模型转换为计算图表示（例如，图 IR）。前端需要实现各种格式转换，以支持不同框架中的各种格式。计算图优化融合了通用编译器和 DL 特定优化的优化技术，这减少了冗余并提高了图 IR 的效率。此类优化可以分为节点级（例如，nop 消除和零维张量消除）、块级（例如，代数简化、算子融合和算子下沉）和数据流级（例如，CSE、DCE、静态内存规划和布局转换）。

9. 后端：将高级 IR 转换为低级 IR，并执行特定于硬件的优化。一方面，它可以直接将高级 IR 转换为第三方工具链（如 LLVM IR），以利用现有基础设施进行通用优化和代码生成。另一方面，它可以利用 DL 模型和硬件特性的先验知识，通过定制的编译器优化，实现更高效的代码生成。常用的特定于硬件的优化包括硬件固有映射、内存分配和获取、内存延迟隐藏、并行化以及面向循环的优化。为了确定大型优化空间中的最优参数设置，现有的 DL 编译器中广泛采用了两种方法，即自动调度（例如多面体模型）和自动调优（例如 AutoTVM）。经过优化的低级 IR 使用 JIT 或 AOT 编译，以生成针对不同硬件目标的代码。

10. 图IR的三种表示：
    
    - 基于 DAG 的 IR：DAG-based IR，DAG 的节点表示原子 DL 算子（卷积、池化等），而边表示张量，并且图是无环无圈的。基于 DAG 的 IR 由于其简单性而便于编程和编译，但它也有缺点，例如由于缺少计算范围的定义而导致的语义歧义。借助 DAG 计算图，DL 编译器可以分析各种算子之间的关系和依赖性，并以此来指导优化。DDG 中已经有很多优化，例如公共子表达式消除 (CSE) 和死代码消除 (DCE)。
    
    - 基于let绑定的IR：Let-binding-based IR，let绑定是解决语义歧义的一种方法，它为许多高级编程语言（如Javascript 、F# 和Scheme ）中的某些函数提供了let表达式，这些函数具有受限的范围。当使用let关键字定义表达式时，会生成一个let节点，然后它指向表达式中的运算符和变量，而不是像DAG那样仅仅构建变量之间的计算关系。在基于 DAG 的编译器中，当一个进程需要获取一个表达式的返回值时，它首先访问对应的节点并搜索相关节点，也称为递归下降技术。相反，基于 let 绑定的编译器计算出 let 表达式中所有变量的结果并构建一个变量映射。当需要特定结果时，编译器会查找此映射来决定表达式的结果。在 DL 编译器中，TVM 的 Relay IR  同时采用了基于 DAG 的 IR 和基于 let 绑定的 IR，以获得两者的优点。
    
    - 张量计算表示：Representing Tensor Computation，不同的图 IR 有不同的方式来表示张量上的计算，各种 DL 框架的操作根据此类特定表示转换为图 IR，分3类：
      
      - [ ] 基于函数：Function-based，基于函数的表示只提供封装的操作符，Glow、nGraph 和 XLA 采用这种表示。以高级优化器（HLO，XLA 的 IR）为例，它由符号编程中的一组函数组成，其中大多数没有副作用。指令组织成三个级别，包括 HloModule（整个程序）、HloComputaion（函数）和 HloInstruction（操作）。
      
      - [ ]  Lambda 表达式：Lambda expression，Lambda 表达式是一种索引公式表达式，通过变量绑定和替换来描述计算。使用 Lambda 表达式，程序员无需实现新函数即可快速定义计算。TVM 使用基于 Lambda 表达式的张量表达式表示张量计算。在 TVM 中，张量表达式中的计算算子由输出张量的形状和计算规则的 Lambda 表达式定义。
      
      - [ ] 爱因斯坦记号：Einstein notation，也称为求和约定，一种表示求和的记号。其编程简洁性优于lambda表达式。以TC为例，临时变量的索引不需要定义。IR可以根据爱因斯坦记号中未定义变量的出现情况，推断出实际表达式。在爱因斯坦记号中，运算符需要具有结合性和交换性。这种限制保证了约简运算符可以按任意顺序执行，从而可以进一步并行化。

11. 图IR的数据表示：Data representation，DL 编译器中的数据（例如，输入、权重和中间数据）通常以张量（也称为多维数组）的形式组织。DL 编译器可以通过内存指针直接表示张量数据，或者通过占位符以更灵活的方式表示。占位符包含张量的每个维度的尺寸。或者，张量的维度尺寸可以标记为未知。对于优化，DL 编译器需要数据布局信息。此外，迭代器的边界应根据占位符进行推断。
    
    - 占位符：Placeholder，占位符广泛用于符号编程（例如 Lisp 、Tensorflow [1]）。占位符只是一个具有明确形状信息（例如每个维度的尺寸）的变量，它将在计算的后期填充值。它允许程序员描述操作并构建计算图，而不必考虑确切的数据元素，这有助于将计算定义与 DL 编译器中的确切执行分开。此外，程序员可以通过使用占位符来更改输入/输出和其他相应中间数据，而无需更改计算定义，这非常方便。
    
    - 未知（动态）形状表示：Unknown (Dynamic) shape representation，声明占位符时通常支持未知维度大小。例如，TVM 使用 Any 表示未知维度（例如，Tensor ⟨(Any, 3), fp32⟩）；XLA 使用 None 实现相同目的（例如，tf.placeholder (“float”, [None, 3])); nGraph 使用其 PartialShape 类。未知形状表示对于支持动态模型是必需的。但是，为了完全支持动态模型，应该放松边界推理和维度检查。此外，应该实现额外的机制来保证内存有效性。
    
    - 数据布局：Data layout，数据布局描述了张量在内存中的组织方式，它通常是逻辑索引到内存索引的映射。数据布局通常包括维度顺序（例如 NCHW 和 NHWC）、平铺、填充、步幅等。TVM 和 Glow 将数据布局表示为算子参数，并需要此类信息进行计算和优化。然而，将数据布局信息与算子而不是张量结合起来，可以为某些算子实现直观的实现，并减少编译开销。
    
    - 边界推理：Bound inference，边界推理用于在 DL 编译器中编译 DL 模型时确定迭代器的边界。虽然 DL 编译器中的张量表示便于描述输入和输出，但它对推断迭代器边界提出了特殊的挑战。边界推理通常根据计算图和已知的占位符递归或迭代执行。一旦根据占位符的形状确定了根迭代器的边界，就可以根据关系递归地推断其他迭代器。

12. 图IR支持的操作：DL 编译器支持的操作负责表示 DL 工作负载，并且它们是计算图的节点。这些操作通常包括代数操作（例如，+、×、exp 和 topK）、神经网络操作（例如，卷积和池化）、张量操作（例如，reshape、resize 和 copy）、广播和归约操作（例如，min 和 argmin），以及控制流操作（例如，条件和循环）。

13. 低级IR的三类实现：低级 IR 以比高级 IR 更细粒度的表示形式描述 DL 模型的计算，通过提供接口来调整计算和内存访问，从而实现目标相关的优化。
    
    - 基于 Halide 的 IR：Halide-based IR，Halide 的基本理念是计算和调度分离。采用 Halide 的编译器不是直接给出特定的方案，而是尝试各种可能的调度并选择最佳的方案。Halide 无法表达具有复杂模式（例如非矩形）的计算。但DL 中的计算非常规则，可以由 Halide 完美地表达。Halide 可以轻松地对这些边界进行参数化，并将它们暴露给调优机制。当应用于 DL 编译器的后端时，需要修改 Halide 的原始 IR。例如，Halide 的输入形状是无限的，而 DL 编译器需要知道数据的准确形状才能将算子映射到硬件指令。一些编译器（例如 TC）需要固定大小的数据，以确保张量数据的更好的时间局部性。
    
    - 基于多面体的 IR：Polyhedral-based IR，它使用线性规划、仿射变换和其他数学方法来优化具有静态控制流边界和分支的基于循环的代码。与 Halide 相反，在多面体模型中，内存引用和循环嵌套的边界可以是具有任何形状的多面体。这种灵活性也阻碍了与调优机制的集成。TC 在低级 IR 中有其独特的设计，它结合了 Halide 和多面体模型。它使用基于 Halide 的 IR 来表示计算，并采用基于多面体的 IR 来表示循环结构。
    
    - 其他独特的IR：一些 DL 编译器实现了定制的低级 IR，而没有使用 Halide 和多面体模型。在定制的低级 IR 上，它们应用了特定于硬件的优化，并降低到 LLVM IR。其中之一便是MLIR，MLIR 深受 LLVM 的影响，并且它比 LLVM 更纯粹的编译器基础设施。MLIR 重用了 LLVM 中的许多想法和接口，并且位于模型表示和代码生成之间。MLIR 具有灵活的类型系统，并允许多个抽象级别，它引入了方言来表示这些多个抽象级别。 每个方言都包含一组定义的不可变操作。MLIR 的当前方言包括 TensorFlow IR、XLA HLO IR、实验多面体 IR、LLVM IR 和 TensorFlow Lite。方言之间也支持灵活的转换。此外，MLIR 可以创建新的方言来连接到新的低级编译器，这为硬件开发人员和编译器研究人员铺平了道路。

14. DL编译器的两类编译方案：
    
    - 即时编译 (JIT)：动态生成可执行代码，并且可以利用更好的运行时信息来优化代码。
    
    - 提前编译 (AOT)：首先生成所有可执行二进制文件，然后执行它们。 因此，它们在静态分析中比 JIT 编译具有更大的范围。此外，AOT 方法可以应用于嵌入式平台的交叉编译器。

15. 前端优化：只应用于计算图，与硬件无关。许多优化在图级别更容易识别和执行，因为图提供了计算的全局视图。本论文分为3类：
    
    - 节点级优化：Node-level optimizations，计算图的节点足够粗糙，以便在单个节点内进行优化。节点级优化包括节点消除，用于消除不必要的节点，以及节点替换，用较低成本的节点替换其他节点。在通用编译器中，Nop消除会移除占用少量空间但不指定任何操作的空操作指令。在DL编译器中，Nop消除负责消除缺乏足够输入的操作。例如，只有一个输入张量的求和节点可以被消除，填充宽度为零的填充节点可以被消除。零维张量消除负责移除输入为零维张量的多余操作。假设 A 是一个零维张量，B 是一个常量张量，那么 A 和 B 的求和操作节点可以用已经存在的常量节点 B 替换，而不影响正确性。
    
    - 块级优化（窥孔优化）：Block-level optimizations，它又分为3类：
      
      - 代数简化：Algebraic simpliﬁcation，包括 1) 代数识别，2) 强度缩减，通过它我们可以用更便宜的运算符替换更昂贵的运算符；3) 常量折叠，通过它我们可以用常量表达式的值替换常量表达式。此类优化考虑节点序列，然后利用不同类型节点的交换律、结合律和分配律来简化计算。除了典型的运算符（+、× 等）之外，代数简化还可以应用于 DL 特定的运算符（例如，reshape、transpose 和 pooling）。可以对运算符重新排序，有时还可以消除它们，这可以减少冗余并提高效率。这里我们说明了可以应用代数简化的常见情况：1）计算顺序优化，在这种情况下，优化会根据特定特征查找并删除 reshape/transpose 运算。以矩阵乘法（GEMM）为例，有两个矩阵（例如，A 和 B），这两个矩阵都经过转置（分别生成 AT 和 BT），然后将 AT 和 BT 相乘。然而，实现 GEMM 的更有效方法是交换参数 A 和 B 的顺序，将它们相乘，然后转置 GEMM 的输出，这将两次转置减少到一次；2) 节点组合优化，在这种情况下，优化将多个连续转置节点组合成一个节点，消除恒等转置节点，并将转置节点优化为 reshape 节点，当它们实际上没有移动数据时；3) ReduceMean 节点优化，在这种情况下，优化执行使用 AvgPool 节点替换 ReduceMean（例如，在 Glow 中），如果 reduce 运算符的输入是 4D，最后两个维度将被缩减。
      
      - 算子融合：Operator fusion，它可以更好地共享计算，消除中间分配，通过组合循环嵌套促进进一步优化，以及减少启动和同步开销 。在 TVM 中，算子分为四类：单射、约简、复杂可融合和不透明。当定义运算符时，确定其对应的类别。针对上述类别，TVM 设计了运算符之间的融合规则。在 TC 中，融合基于自动多面体变换以不同的方式执行。然而，如何识别和融合更复杂的图模式，例如具有多个广播和归约节点的块，仍然是一个问题。
      
      - 算子下沉：Operator sinking，此优化将转置等操作下沉到批归一化、ReLU、sigmoid 和通道混洗等操作下方。通过此优化，许多类似的操作被移得更近，从而创造了更多代数简化的机会。
    
    - 数据流级优化（全局优化）：Dataflow-level optimizations，它又分为4类：
      
      - 公共子表达式消除（CSE）：表达式 E 是一个公共子表达式，如果 E 的值之前已计算，并且 E 的值自上次计算以来无需更改 [6]。在这种情况下，E 的值计算一次，并且 E 的已计算值可用于避免在其他地方重新计算。DL 编译器通过整个计算图搜索公共子表达式，并将以下公共子表达式替换为先前计算的结果。
      
      - 死代码消除 (DCE)：如果计算结果或副作用未被使用，则代码集是死的。DCE 优化会移除死代码。死代码通常不是由程序员造成的，而是由其他图形优化造成的。因此，DCE 和 CSE 会在其他图优化之后应用。其他优化，例如死存储消除 (DSE)，它会移除永远不会被使用的张量中的存储，也属于 DCE。
      
      - 静态内存规划：Static memory planning，静态内存规划优化用于尽可能地重用内存缓冲区。通常，有两种方法：就地内存共享和标准内存共享。就地内存共享对操作使用相同的输入和输出内存，并且仅在计算前分配一份内存副本。标准的内存共享在不重叠的情况下重复使用先前操作的内存。静态内存规划是在线完成的，这允许应用更复杂的规划算法。
      
      - 布局转换：Layout transformation，布局转换尝试找到存储计算图中张量的最佳数据布局，然后将布局转换节点插入到图中。请注意，实际转换不会在此处执行，而是将在编译器后端评估计算图时执行。事实上，相同操作在不同数据布局中的性能是不同的，并且最佳布局在不同的硬件上也是不同的。编译器需要提供一种方法来执行各种硬件上的布局转换。

16. 后端优化：深度学习编译器的后端通常包含各种特定于硬件的优化、自动调整技术和经过优化的内核库。特定于硬件的优化能够为不同的硬件目标生成高效的代码。而自动调整对于编译器后端来说至关重要，它可以减轻手动获取最优参数配置的工作量。此外，高度优化的内核库也广泛用于通用处理器和其他定制的 DL 加速器。
    
    - 硬件特定优化：Hardware-specific Optimization，也称为目标相关优化，用于获取针对特定硬件的高性能代码。应用后端优化的一个方法是将低级 IR 转换为 LLVM IR，以利用 LLVM 基础设施生成优化的 CPU/GPU 代码。另一种方法是利用深度学习领域知识设计定制优化，更有效地利用目标硬件。目前经常使用的5种方法：
      
      - 硬件固有映射：Hardware intrinsic mapping，将一组特定的低级 IR 指令转换为在硬件上已经高度优化的内核。这种方法使得编译器后端能够将硬件实现以及高度优化的微内核应用到特定的操作模式，从而显著提升性能。Halide/TVM 将特定 IR 模式映射到每个架构上的 SIMD 操作码，以避免在遇到向量模式时 LLVM IR 映射的低效率。在 TVM 中，硬件固有映射在可扩展张量化方法中实现，该方法可以声明硬件固有的行为和固有映射的降低规则。
      
      - 内存分配和获取：Memory allocation and fetching，GPU 主要包含共享内存空间（访问延迟较低，但内存大小有限）和本地内存空间（访问延迟较高，但容量较大）。这种内存层次结构需要高效的内存分配和获取技术来改善数据局部性。为了实现此优化，TVM 引入了内存范围调度概念。内存范围调度基元可以将计算阶段标记为共享或线程局部。对于标记为共享的计算阶段，TVM 生成了具有共享内存分配和协作数据获取的代码，它在适当的代码位置插入内存屏障以保证正确性。
      
      - 内存延迟隐藏：Memory latency hiding，由于大多数 DL 编译器支持 CPU 和 GPU 上的并行化，因此可以通过硬件（例如，GPU 上的 warp 上下文切换）自然实现内存延迟隐藏。但是对于具有解耦访问执行 (DAE) 架构的类似 TPU 的加速器，后端需要执行调度和细粒度同步以获取正确且高效的代码。为了获得更好的性能并减少编程负担，TVM 引入了虚拟线程调度基元，它使用户能够在虚拟化多线程架构上指定数据并行性。然后，TVM 通过插入必要的内存屏障降低这些虚拟并行线程，并将这些线程中的操作交错到一个指令流中，该指令流形成每个线程更好的执行管道以隐藏内存访问延迟。
      
      - 循环优化：Loop oriented optimizations，主要包括5种：
        
        - [ ] 循环融合：Loop fusion，融合具有相同边界的循环以实现更好的数据重用。对于 PlaidML、TVM、TC 和 XLA 等编译器，此类优化由 Halide 调度或多面体方法执行，而 Glow 通过其算子堆叠应用循环融合。
        
        - [ ]  滑动窗口：Sliding windows，核心思想是在需要时计算值，并在数据重用时动态存储它们，直到不再需要它们为止。由于滑动窗口交织了两个循环的计算并使它们串行化，因此这是并行性和数据重用之间的权衡。
        
        - [ ]  平铺：Tiling，将循环拆分为多个平铺，因此循环被划分为遍历平铺的外循环和在平铺内遍历的内循环。这种转换通过将平铺放入硬件缓存中，从而在平铺内实现更好的数据局部性。由于平铺的大小是特定于硬件的，因此许多 DL 编译器通过自动调整来确定平铺模式和大小。
        
        - [ ] 循环重排：Loop reordering，也称为循环排列，更改嵌套循环中迭代的顺序，这可以优化内存访问，从而提高空间局部性。它特定于数据布局和硬件功能。但是，当沿迭代顺序存在依赖关系时，执行循环重排是不安全的。
        
        - [ ]  循环展开：Loop unrolling，将特定循环展开为固定数量的循环体副本，这允许编译器应用激进的指令级并行性。通常，循环展开与循环拆分结合使用，后者首先将循环拆分为两个嵌套循环，然后完全展开内部循环。
      
      - 并行化：Parallelization，由于现代处理器通常支持多线程和 SIMD 并行化，因此编译器后端需要利用并行化来最大化硬件利用率以获得高性能。完全利用编译器后端并行性可以应用更多 DL 模型的领域特定知识，因此以更多工程工作为代价获得了更高的性能。
    
    - 自动调优：Auto-tuning，由于硬件特定优化中参数调整的搜索空间巨大，因此有必要利用自动调整来确定最佳参数配置。在本论文中研究的 DL 编译器中，TVM、TC 和 XLA 支持自动调整。一般来说，自动调优实现包括四个关键组件，如参数化、成本模型、搜索技术和加速。
      
      - 参数化：Parameterization，数据参数描述了数据的规范，例如输入形状。目标参数描述了在优化调度和代码生成期间需要考虑的特定于硬件的特性和约束。优化选项包括优化调度和相应参数，例如面向循环的优化和切片大小。
      
      - 成本模型：Cost model，分为3类：
        
        - 黑盒模型：Black-box model，仅考虑最终执行时间，而不考虑编译任务的特征。构建黑盒模型很容易，但如果没有任务特征的指导，很容易导致更高的开销和不太理想的解决方案。TC 采用此模型。
        
        - 基于 ML 的成本模型：ML-based cost model，基于 ML 的成本模型是一种使用机器学习方法预测性能的统计方法。它使模型能够在探索新配置时进行更新，这有助于实现更高的预测精度。例如，TVM 和 XLA 采用这种模型，分别是梯度树提升模型 (GBDT) 和前馈神经网络(FNN)。
        
        - 预定义成本模型：Predeﬁned cost model，基于预定义成本模型的方法期望建立一个基于编译任务特征的完美模型，并且能够评估任务的整体性能。与基于 ML 的模型相比，预定义模型在应用时产生的计算开销更少，但需要大量的工程工作才能在每个新的 DL 模型和硬件上重新构建模型。
      
      - 搜索技术：Searching technique，首先进行初始化和搜索空间确定。初始选项可以随机设置，也可以基于已知配置，例如用户提供的配置或历史最佳配置。在搜索空间方面，它应该在自动调整之前指定。TVM 允许开发人员使用其特定领域的知识来指定搜索空间，并根据计算描述为每个硬件目标提供自动搜索空间提取。相比之下，TC 依赖于编译缓存和预定义规则。优化算法有3类：
        
        - 遗传算法 (GA)：将每个调整参数视为基因，并将每个配置视为候选。新的候选者是通过交叉、变异和选择根据适应度值迭代生成的，适应度值是一种受自然选择过程启发的元启发式算法。最后，得出最佳候选者。交叉、变异和选择的比率用于控制探索和利用之间的权衡。TC 在其自动调整技术中采用了 GA。
        
        -  模拟退火算法 (SA)：SA 也是一种受退火启发的元启发式算法。它允许我们以递减的概率接受更差的解决方案，这可以在固定数量的迭代中找到近似全局最优解，并避免精确的局部最优解。TVM 在其自动调整技术中采用了 SA。
        
        - 强化学习 (RL)：RL 通过探索和利用之间的权衡，在给定环境的情况下学习最大化奖励。变色龙（基于 TVM 构建）在其自动调优技术中采用了 RLRL。
      
      - 加速：Acceleration，分为2个方向：
        
        - 并行化：Parallelization，TC 提出了一种多线程、多 GPU 策略，因为它考虑到了遗传算法需要评估每一代中的所有候选者。首先，它将候选配置排队，并在多个 CPU 线程上编译它们。生成的代码在 GPU 上并行评估，每个候选者拥有其在父选择步骤中使用的适应度。在完成整个评估后，生成新的候选者，并将新的编译作业排队，等待在 CPU 上编译。类似地，TVM 支持交叉编译和 RPC，允许用户在本地机器上编译并在多个目标上使用不同的自动调优配置运行程序。
        
        - 配置重用：Conﬁguration reuse，重用先前的自动调优配置。TC 通过编译缓存存储给定配置对应的已知最快生成代码版本。在编译期间每次内核优化之前查询缓存，如果缓存未命中，则触发自动调优。类似地，TVM 会生成一个日志文件，用于存储所有调度操作符的最佳配置，并在编译期间查询日志文件以获取最佳配置。值得一提的是，TVM 对 Halide IR 中的每个算子（例如 conv2d）执行自动调整，因此为每个算子分别确定最佳配置。
    
    - 优化内核库：Optimized Kernel Libraries，当计算可以通过特定高度优化的基元满足时，使用优化的内核库可以实现显著的性能提升，否则可能会受到进一步优化的限制，并遭受不太理想的性能。

---

## 个人思考

与传统编译器相比，许多基本概念思路（模块化设计、关注点分离原则、IR设计等）与优化都是相似的，多的主要是DL特有的优化（算子融合、内核融合等）以及更加针对DL硬件特性的优化。本论文给我的主要启发是文章最后给出的9个未来方向，我认为这对于我的未来科研工作是具有启发性的。这9个方向分别是：

---

1. 动态形状和前/后处理（Dynamic shape and pre/post processing）

2. 高级自动调优（Advanced auto-tuning）

3. 多面体模型（Polyhedral model）

4. 子图划分（Subgraph partitioning）

5. 量化（Quantization）

6. 统一优化（Uniﬁed optimizations）

7. 可微编程（Diﬀerentiable programming）

8. 隐私保护（Privacy protection）

9. 训练支持（Training support）
