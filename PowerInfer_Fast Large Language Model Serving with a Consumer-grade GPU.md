# PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU

# PowerInfer: 基于消费级GPU的快速大型语言模型服务

---

## 关键词

局部性、GPU、CPU、神经元

---

## 核心思想

利用 LLM 推理中的局部性，将少量热神经元分配给 GPU，而将占大多数的冷神经元由 CPU 管理。

---

## 主要工作

提出PowerInfer，一款针对大型语言模型 (LLM) 优化的快速推理系统，它利用了 LLM 推理中的局部性属性。它使用自适应预测器和神经元感知算子来实现神经元激活和计算稀疏性。

---

## 学到的知识点

1. **PowerInfer**：个在配备单个消费级 GPU 的个人计算机 (PC) 上运行的高速大型语言模型 (LLM) 推理引擎。设计的关键在于利用 LLM 推理中固有的高局部性，其特征是神经元激活的幂律分布。这种分布表明，一小部分神经元（称为热神经元）在不同输入中始终被激活，而大多数神经元（称为冷神经元）则根据特定输入而变化。PowerInfer 利用这一见解设计了一个 GPU-CPU 混合推理引擎：热激活神经元预加载到 GPU 上以实现快速访问，而冷激活神经元则在 CPU 上计算，从而显著减少了 GPU 内存需求和 CPU-GPU 数据传输。并进一步整合了自适应预测器和神经元感知稀疏算子，优化了神经元激活和计算稀疏性的效率。为了减少推理延迟，推理引擎仅计算在线预测器预测为活跃的神经元，跳过大多数非活跃的神经元。此外，预加载策略使 PowerInfer 能够将大部分推理任务分配给 GPU，因为已加载到 GPU 上的热激活神经元构成了激活的大部分。对于不在 GPU 内存中的冷激活神经元，PowerInfer 在 CPU 上执行它们的计算，从而避免了将权重传输到 GPU 的需要。

2. 大型语言模型（LLM）：一种自回归模型，对内存的需求量巨大，将其部署在消费级 GPU 上存在重大挑战。LLM 通常作为自回归 Transformer，以逐个词元的顺序生成文本，每个词元都需要访问包含数千亿个参数的整个模型。因此，推理过程本质上受限于 GPU 的内存容量。这种限制在本地部署中尤其突出，因为对单个请求（通常一次只有一个）[6] 的处理几乎没有并行处理的机会。

3. 解决大型语言模型（LLM）对内存的需求量巨大的2种方法：
   
   - 模型压缩：model compression，压缩技术如量化、蒸馏和剪枝可以减小模型尺寸。然而，即使深度压缩后的模型对于消费级 GPU 来说仍然太大。
   
   - 模型卸载：在 Transformer 层级将模型划分为 GPU 和 CPU。利用两者进行推理，从而减少了所需的 GPU 资源。然而，这种方法受到 PCIe 互连速度慢和 CPU 计算能力有限的阻碍，导致推理延迟较高。2种主要卸载方法：
     
     - 以 GPU 为中心的卸载：GPU-centric ofﬂoading，利用 CPU 内存来存储超过 GPU 容量的模型参数部分。在每次迭代中，它处理位于 GPU 内存中的参数，根据需要从 CPU 传输更多参数。这种策略能够推断出不同大小的LLM，前提是拥有足够的CPU内存和硬盘存储空间。然而，这种方法在延迟敏感场景下会导致显著的每个token延迟，主要是由于GPU和CPU之间频繁的数据传输，尤其是在批次大小为1的情况下。超过99.5%的处理时间被消耗在将LLM权重从CPU传输到GPU上，这显著影响了整体延迟。
     
     - 混合卸载：Hybrid ofﬂoading，将模型参数分布在 GPU 和 CPU 之间，在 Transformer 层级进行分割。CPU 首先处理其层，然后将中间结果发送到 GPU 以进行令牌生成。这种卸载方法通过最小化数据传输和缓解缓慢的 PCIe 带宽。然而，混合卸载仍然面临着局部性不匹配问题，导致延迟不理想。每次推理迭代都会访问整个模型，导致分层 GPU-CPU 内存结构的局部性较差。GPU 虽然计算能力强大，但受限于内存容量。

4. **LLM 推理中内存问题的主要原因**：硬件架构与 LLM 推理特性之间的局部性不匹配。当前的硬件架构设计了针对数据局部性优化的内存层次结构。理想情况下，一个小而频繁访问的工作集应该存储在 GPU 中，GPU 提供更高的内存带宽，但容量有限。相反，更大、访问频率较低的数据更适合存储在 CPU 中，CPU 提供更大的内存容量，但带宽较低。然而，每个 LLM 推理迭代所需的庞大参数量导致工作集过大，无法容纳在一个 GPU 中，从而阻碍了高效的局部性利用。

5. **LLM 推理固有地表现出高度局部性（PowerInfer的灵感）**：在每次推理迭代中，只有有限数量的神经元 被激活，这对标记推理的结果有重大影响。这些激活是特定于输入的，可以在运行时准确预测。大型语言模型 (LLM) 中的神经元激活遵循**偏斜的幂律分布**：一小部分神经元在各种输入中始终贡献着大部分激活（超过 80%）（热激活），而大多数神经元参与剩余的激活，这些激活是在运行时根据输入确定的（冷激活）。2个独特特征：
   
   - 幂律激活：Power-law Activation，LLM 推理表现出高度的局部性，表明一组一致的神经元经常被激活。尽管 LLM 激活稀疏性依赖于输入，但在激活的神经元中仍然存在幂律分布。这种高局部性并不局限于单层，而是贯穿整个模型。
   
   - 快速 CPU 内计算：Fast In-CPU Computation，如果激活的神经元驻留在 CPU 内存中，则在 CPU 上计算它们比将它们传输到 GPU 更快，尤其是在本地部署中，激活的神经元数量很少，批次大小也很小。具有向量扩展的现代 CPU 可以有效地处理这种较小的矩阵计算。

6. **PowerInfer的设计核心**：利用 **LLM 推理中的局部性**，将少量热神经元分配给 GPU，而将占大多数的冷神经元由 CPU 管理。PowerInfer 在离线阶段预选并预加载热激活神经元到 GPU 上，并在运行时利用在线预测器来识别激活的神经元。这种方法允许 GPU 和 CPU 独立处理各自的神经元集，从而最大限度地减少了对昂贵的 PCIe 数据传输的需求。

7. PowerInfer的离线与在线组件：
   
   - LLM 分析器和策略求解器（离线）：LLM Proﬁler and Policy Solver (Ofﬂine)，该组件包含一个 LLM 分析器，它使用从通用数据集，派生的请求从推理过程中收集激活数据。它监控所有层的神经元激活，然后由策略求解器将神经元分类为热或冷。求解器旨在将频繁激活的神经元分配到 GPU，而将其他神经元分配到 CPU。它使用神经元影响指标和硬件规格来平衡工作负载，使用整数线性规划来最大化 GPU 对神经元的影响指标。
   
   - 神经元感知 LLM 推理引擎（在线）：Neuron-aware LLM Inference Engine (Online)，在线引擎在处理用户请求之前，根据离线求解器的输出，将两种类型的神经元分配到各自的处理单元。在运行时，引擎创建 GPU 和 CPU 执行器，它们是运行在 CPU 端的线程，用于管理并发的 CPU-GPU 计算。该引擎还可以预测神经元的激活并跳过未激活的神经元。激活的神经元预加载在GPU 内存在此处进行处理，而 CPU 计算并传输结果以供其神经元整合到 GPU。该引擎在 CPU 和 GPU 上都使用稀疏神经元感知算子，专注于矩阵中单个神经元行/列。

8. **重要的是要注意，通过离线统计分析识别出的热激活神经元可能与运行时激活行为不一致。**

9. 预测器的大小受两个主要因素的影响：
   
   - LLM 层的稀疏性：具有较高激活稀疏性的层简化了识别激活神经元的任务，从而允许使用更小的预测器模型。相反，具有较低激活稀疏性的层需要更大的模型，即具有更多参数，因为准确地识别激活的神经元变得越来越困难。
   
   - LLM 层的内部偏斜：在高度偏斜的情况下，即激活集中在少数神经元中，即使是紧凑的预测器也能实现高精度。

10. **GPU-CPU 混合执行**：GPU-CPU Hybrid Execution，其中两个单元独立计算各自的激活神经元，然后在 GPU 上组合结果。该方法有效地平衡了计算工作量，利用了每个单元的优势，同时减少了传输时间低效。

11. 神经元感知算子：Neuron-aware Operator，2种：
    
    - 面向 GPU 的神经元感知算子：Neuron-aware Operators for GPU，尽管在 GPU 上向量-向量计算效率低于矩阵-向量计算，但基于向量-向量计算的神经元感知算子在批次大小较小时具有优势。它们避免了与非活动神经元相关的无用计算和内存操作，并且不需要代价高昂的矩阵转换。此外，这些运算符允许所有线程块同时检查神经元激活并计算相应的向量（如果激活）。
    
    - 面向神经元的 CPU 算子：Neuron-aware Operators for CPU，面向神经元的算子对于 CPU 尤其有利，因为 CPU 通常具有较低的并行性和矩阵计算效率。CPU 执行器将面向神经元的算子分配给多个核心，将神经元划分为更小的批次以进行并发激活检查。每个核心只处理其批次中激活的神经元，并使用硬件向量扩展（如 AVX2）优化向量-向量计算，AVX2 在现代 CPU 中得到广泛支持。

---

## 个人思考

PowerInfer的设计思路为大型语言模型（LLM）推理提供了一种创新的方法，尤其是在资源受限的消费级GPU环境中。其核心在于充分利用神经元激活的局部性特征，将活跃的“热神经元”置于GPU中，而将大部分不活跃的“冷神经元”由CPU管理。这种GPU-CPU混合执行的策略有效降低了内存需求和数据传输延迟，克服了传统推理方法中的局部性不匹配问题。

从技术角度来看，PowerInfer的自适应预测器和神经元感知算子在提升推理效率方面起到了关键作用。通过离线分析和在线优化，该系统能够动态识别活跃神经元，最大限度地减少不必要的计算。这种方法不仅提升了处理速度，还显著降低了在小批量推理时的延迟，为实际应用提供了更流畅的体验。

此外，PowerInfer的设计也反映了当前深度学习领域对硬件资源优化的趋势。在LLM日益庞大的背景下，如何在保证性能的同时高效利用计算资源，已经成为一个重要课题。通过有效分配资源，PowerInfer不仅增强了LLM在消费级设备上的可行性，还为未来的推理系统设计提供了宝贵的思路。这种混合计算的模式可能会激励更多研究者关注如何在复杂模型中优化计算流程，为推动人工智能的广泛应用奠定基础。
